{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named input_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-25d5142a32fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# input_data.pyはチュートリアル内にリンクがあるのでそこから取得する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/examples/tutorials/mnist/input_data.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named input_data"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# TensowFlowのインポート\n",
    "import tensorflow as tf\n",
    "# MNISTを読み込むためinput_data.pyを同じディレクトリに置きインポートする\n",
    "# input_data.pyはチュートリアル内にリンクがあるのでそこから取得する\n",
    "# https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "import input_data\n",
    "\n",
    "import time\n",
    "\n",
    "# 開始時刻\n",
    "start_time = time.time()\n",
    "print \"開始時刻: \" + str(start_time)\n",
    "\n",
    "# MNISTデータの読み込み\n",
    "# 60000点の訓練データ（mnist.train）と10000点のテストデータ（mnist.test）がある\n",
    "# 訓練データとテストデータにはそれぞれ0-9の画像とそれに対応するラベル（0-9）がある\n",
    "# 画像は28x28px(=784)のサイズ\n",
    "# mnist.train.imagesは[60000, 784]の配列であり、mnist.train.lablesは[60000, 10]の配列\n",
    "# lablesの配列は、対応するimagesの画像が3の数字であるならば、[0,0,0,1,0,0,0,0,0,0]となっている\n",
    "# mnist.test.imagesは[10000, 784]の配列であり、mnist.test.lablesは[10000, 10]の配列\n",
    "print \"--- MNISTデータの読み込み開始 ---\"\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print \"--- MNISTデータの読み込み完了 ---\"\n",
    "\n",
    "# 訓練画像を入れる変数\n",
    "# 訓練画像は28x28pxであり、これらを1行784列のベクトルに並び替え格納する\n",
    "# Noneとなっているのは訓練画像がいくつでも入れられるようにするため\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# 重み\n",
    "# 訓練画像のpx数の行、ラベル（0-9の数字の個数）数の列の行列\n",
    "# 初期値として0を入れておく\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "# バイアス\n",
    "# ラベル数の列の行列\n",
    "# 初期値として0を入れておく\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# ソフトマックス回帰を実行\n",
    "# yは入力x（画像）に対しそれがある数字である確率の分布\n",
    "# matmul関数で行列xとWの掛け算を行った後、bを加算する。\n",
    "# yは[1, 10]の行列\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 交差エントロピー\n",
    "# y_は正解データのラベル\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# 勾配硬化法を用い交差エントロピーが最小となるようyを最適化する\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# 用意した変数Veriableの初期化を実行する\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Sessionを開始する\n",
    "# runすることで初めて実行開始される（run(init)しないとinitが実行されない）\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 1000回の訓練（train_step）を実行する\n",
    "# next_batch(100)で100つのランダムな訓練セット（画像と対応するラベル）を選択する\n",
    "# 訓練データは60000点あるので全て使いたいところだが費用つまり時間がかかるのでランダムな100つを使う\n",
    "# 100つでも同じような結果を得ることができる\n",
    "# feed_dictでplaceholderに値を入力することができる\n",
    "print \"--- 訓練開始 ---\"\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_:batch_ys})\n",
    "print \"--- 訓練終了 ---\"\n",
    "\n",
    "# 正しいかの予測\n",
    "# 計算された画像がどの数字であるかの予測yと正解ラベルy_を比較する\n",
    "# 同じ値であればTrueが返される\n",
    "# argmaxは配列の中で一番値の大きい箇所のindexが返される\n",
    "# 一番値が大きいindexということは、それがその数字である確率が一番大きいということ\n",
    "# Trueが返ってくるということは訓練した結果と回答が同じということ\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "# 精度の計算\n",
    "# correct_predictionはbooleanなのでfloatにキャストし、平均値を計算する\n",
    "# Trueならば1、Falseならば0に変換される\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# 精度の実行と表示\n",
    "# テストデータの画像とラベルで精度を確認する\n",
    "# ソフトマックス回帰によってWとbの値が計算されているので、xを入力することでyが計算できる\n",
    "print \"精度\"\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "# 終了時刻\n",
    "end_time = time.time()\n",
    "print \"終了時刻: \" + str(end_time)\n",
    "print \"かかった時間: \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 images belonging to 2 classes.\n",
      "Found 41 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      " 728/1680 [============>.................] - ETA: 29s - loss: 0.1420 - acc: 0.7720\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os.path\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 56, 56\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "\n",
    "f_log = './log'\n",
    "#model saving location\n",
    "f_model = './model'\n",
    "model_filename = 'cnn_model.json'\n",
    "# weights_filename = 'cnn_model_weights.hdf5' \n",
    "weights_filename = 'weights.hdf5'\n",
    "\n",
    "train_sample_number = 56\n",
    "test_sample_number = 41\n",
    "\n",
    "nb_train_samples = train_sample_number*30\n",
    "nb_validation_samples = test_sample_number*20\n",
    "train_batch = train_sample_number\n",
    "test_batch = test_sample_number\n",
    "nb_epoch = 20\n",
    "\n",
    "\n",
    "#training model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3,border_mode='valid', input_shape=(img_width, img_height, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=5e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    zca_whitening=True,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=180, \n",
    "    width_shift_range=0.3,\n",
    "\theight_shift_range=0.3,\n",
    "\tfill_mode='nearest',\n",
    "\tvertical_flip=True,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(\n",
    "\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=180, \n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=train_batch,\n",
    "\n",
    "\t# save_to_dir=\"saved\",   \n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=test_batch,\n",
    "        class_mode='binary')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./saved/weights.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples,\n",
    "        callbacks=[checkpointer])\n",
    "\n",
    "\n",
    "print('save the architecture of a model')\n",
    "json_string = model.to_json()\n",
    "open(os.path.join(f_model,'cnn_model.json'), 'w').write(json_string)\n",
    "yaml_string = model.to_yaml()\n",
    "open(os.path.join(f_model,'cnn_model.yaml'), 'w').write(yaml_string)\n",
    "print('save weights')\n",
    "model.save_weights(os.path.join(f_model,'cnn_model_weights.hdf5'))\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_train shape:', (50000, 32, 32, 3))\n",
      "('y_train shape:', (50000, 1))\n",
      "(50000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "# x_train shape: (50000, 32, 32, 3)\n",
    "# y_train shape: (50000, 1)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import urllib.parse\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Turnes every image file to 28*28\n",
    "    Also invert color\n",
    "    \"\"\"\n",
    "\n",
    "iid = count()\n",
    "def random_id(length):\n",
    "    number = '0123456789'\n",
    "    alpha = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    id = ''\n",
    "    for i in range(0,length,2):\n",
    "        id += random.choice(number)\n",
    "        id += random.choice(alpha)\n",
    "    return id\n",
    "\n",
    "def resizer(cur_path):\n",
    "    for a in os.listdir(cur_path):\n",
    "        if(os.path.isdir('%s/%s'%(cur_path,a))):\n",
    "            # resizer('%s/%s'%(cur_path,a))\n",
    "            print('%s/%s'%(cur_path,str(a)))\n",
    "        else:\n",
    "\n",
    "            # img = Image.open('%s/%s'%(cur_path,a), 'r')\n",
    "            # newNameTag= random_id(6)\n",
    "            # newName= newNameTag + str(next(iid)) + \".png\"\n",
    "            # os.rename('%s/%s'%(cur_path,a),'%s/%s'%(cur_path,newName))\n",
    "            try:\n",
    "                img = Image.open('%s/%s'%(cur_path,a), 'r')\n",
    "                newNameTag= random_id(6)\n",
    "                newName= newNameTag + str(next(iid)) + \".png\"\n",
    "                os.rename('%s/%s'%(cur_path,a),'%s/%s'%(cur_path,newName))\n",
    "                \n",
    "            except:\t\t\t\t\n",
    "                print(\"file not image \",str(a))\n",
    "\n",
    "\n",
    "resize_size=(56,56)\n",
    "cur_path = os.path.dirname(os.path.abspath(__file__))\n",
    "resizer(cur_path+\"/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 56, 3)\n",
      "big array=\n",
      "(1, 56, 56, 3)\n",
      "(56, 56, 3)\n",
      "big array=\n",
      "(2, 56, 56, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "dir= \"data/train/1/0f8j3b2.png\"\n",
    "dir1= \"data/train/1/0k7o4v7.png\"\n",
    "\n",
    "big_array = []\n",
    "im = cv2.imread(dir)\n",
    "if im.shape[0] != 56 and im.shape[1] != 56:\n",
    "    im = cv2.resize(im, (56, 56))\n",
    "im = im.tolist()\n",
    "nparray = np.asarray(im)\n",
    "print(nparray.shape)\n",
    "big_array.append(im)\n",
    "print(\"big array=\")\n",
    "print(np.asarray(big_array).shape)\n",
    "\n",
    "\n",
    "\n",
    "dir1= \"data/train/1/0k7o4v7.png\"\n",
    "\n",
    "im1 = cv2.imread(dir1)\n",
    "im1 = im1.tolist()\n",
    "nparray = np.asarray(im1)\n",
    "print(nparray.shape)\n",
    "big_array.append(im1)\n",
    "\n",
    "print(\"big array=\")\n",
    "print(np.asarray(big_array).shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/build/opencv-SviWsf/opencv-2.4.9.1+dfsg/modules/imgproc/src/imgwarp.cpp:1834: error: (-215) ssize.area() > 0 in function resize\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e6c32746ff43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"data/train/1/0f8j3b2.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image_as_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image_as_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".DS_store\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-e6c32746ff43>\u001b[0m in \u001b[0;36mread_image_as_array\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#     if im.shape[0] != 56 and im.shape[1] != 56:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#         im = cv2.resize(im, (56, 56))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /build/opencv-SviWsf/opencv-2.4.9.1+dfsg/modules/imgproc/src/imgwarp.cpp:1834: error: (-215) ssize.area() > 0 in function resize\n"
     ]
    }
   ],
   "source": [
    "def read_image_as_array(dir):\n",
    "    \"\"\"\n",
    "    :param dir: directory of image name\n",
    "    :return array\n",
    "    \"\"\"\n",
    "\n",
    "    im = cv2.imread(dir)\n",
    "    #     if im.shape[0] != 56 and im.shape[1] != 56:\n",
    "    #         im = cv2.resize(im, (56, 56))\n",
    "    im = cv2.resize(im, (56, 56))\n",
    "    im = im.tolist()\n",
    "    return im\n",
    "\n",
    "dir= \"data/train/1/0f8j3b2.png\"\n",
    "print(len(read_image_as_array(dir)))\n",
    "print(len(read_image_as_array(\".DS_store\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not folder, .DS_Store\n",
      "labeling class_name/kerasTest/data/train/0as 1\n",
      "(1, 56, 56, 3)\n",
      "(2, 56, 56, 3)\n",
      "(3, 56, 56, 3)\n",
      "(4, 56, 56, 3)\n",
      "(5, 56, 56, 3)\n",
      "(6, 56, 56, 3)\n",
      "(7, 56, 56, 3)\n",
      "(8, 56, 56, 3)\n",
      "(9, 56, 56, 3)\n",
      "(10, 56, 56, 3)\n",
      "(11, 56, 56, 3)\n",
      "(12, 56, 56, 3)\n",
      "(13, 56, 56, 3)\n",
      "(14, 56, 56, 3)\n",
      "(15, 56, 56, 3)\n",
      "(16, 56, 56, 3)\n",
      "(17, 56, 56, 3)\n",
      "(18, 56, 56, 3)\n",
      "(19, 56, 56, 3)\n",
      "(20, 56, 56, 3)\n",
      "(21, 56, 56, 3)\n",
      "(22, 56, 56, 3)\n",
      "(23, 56, 56, 3)\n",
      "(24, 56, 56, 3)\n",
      "(25, 56, 56, 3)\n",
      "(26, 56, 56, 3)\n",
      "(27, 56, 56, 3)\n",
      "(28, 56, 56, 3)\n",
      "(29, 56, 56, 3)\n",
      "(30, 56, 56, 3)\n",
      "(31, 56, 56, 3)\n",
      "(32, 56, 56, 3)\n",
      "(33, 56, 56, 3)\n",
      "(34, 56, 56, 3)\n",
      "(35, 56, 56, 3)\n",
      "(36, 56, 56, 3)\n",
      "(37, 56, 56, 3)\n",
      "(38, 56, 56, 3)\n",
      "(39, 56, 56, 3)\n",
      "(40, 56, 56, 3)\n",
      "(41, 56, 56, 3)\n",
      "(42, 56, 56, 3)\n",
      "(43, 56, 56, 3)\n",
      "(44, 56, 56, 3)\n",
      "(45, 56, 56, 3)\n",
      "(46, 56, 56, 3)\n",
      "(47, 56, 56, 3)\n",
      "(48, 56, 56, 3)\n",
      "(49, 56, 56, 3)\n",
      "(50, 56, 56, 3)\n",
      "(51, 56, 56, 3)\n",
      "(52, 56, 56, 3)\n",
      "(53, 56, 56, 3)\n",
      "(54, 56, 56, 3)\n",
      "(55, 56, 56, 3)\n",
      "(56, 56, 56, 3)\n",
      "(57, 56, 56, 3)\n",
      "labeling class_name/kerasTest/data/train/1as 2\n",
      "(58, 56, 56, 3)\n",
      "(59, 56, 56, 3)\n",
      "(60, 56, 56, 3)\n",
      "(61, 56, 56, 3)\n",
      "(62, 56, 56, 3)\n",
      "(63, 56, 56, 3)\n",
      "(64, 56, 56, 3)\n",
      "(65, 56, 56, 3)\n",
      "(66, 56, 56, 3)\n",
      "(67, 56, 56, 3)\n",
      "(68, 56, 56, 3)\n",
      "(69, 56, 56, 3)\n",
      "(70, 56, 56, 3)\n",
      "(71, 56, 56, 3)\n",
      "(72, 56, 56, 3)\n",
      "(73, 56, 56, 3)\n",
      "(74, 56, 56, 3)\n",
      "(75, 56, 56, 3)\n",
      "(76, 56, 56, 3)\n",
      "(77, 56, 56, 3)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/build/opencv-SviWsf/opencv-2.4.9.1+dfsg/modules/imgproc/src/imgwarp.cpp:1834: error: (-215) ssize.area() > 0 in function resize\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a7bd98f146f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mdata_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mcur_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# The data, shuffled and split between train and test sets:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a7bd98f146f0>\u001b[0m in \u001b[0;36mdata_generate\u001b[0;34m(cur_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_label_for_each_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_label_for_each_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a7bd98f146f0>\u001b[0m in \u001b[0;36mmake_label_for_each_directory\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpic_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path_class_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mabs_path_pic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs_path_class_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpic_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mpic_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image_as_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path_pic_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mbig_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a7bd98f146f0>\u001b[0m in \u001b[0;36mread_image_as_array\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m#     if im.shape[0] != 56 and im.shape[1] != 56:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m#         im = cv2.resize(im, (56, 56))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /build/opencv-SviWsf/opencv-2.4.9.1+dfsg/modules/imgproc/src/imgwarp.cpp:1834: error: (-215) ssize.area() > 0 in function resize\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "# import urllib.parse # for python 3\n",
    "from urlparse import urlparse # for python 2\n",
    "from itertools import count\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "data/\n",
    "    train/\n",
    "        0/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        1/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        0/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        1/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "\"\"\"\n",
    "\n",
    "def make_label_for_each_directory(dir):\n",
    "    \"\"\"\n",
    "    :param dir\n",
    "    :return (big_array, label_array): (nparray, label)\n",
    "    \"\"\"\n",
    "\n",
    "    big_array = []\n",
    "    label_array = []\n",
    "    for index, class_name in enumerate(os.listdir(dir)):\n",
    "        abs_path_class_name = dir + \"/\" + class_name\n",
    "        if(os.path.isdir(abs_path_class_name)):\n",
    "            print(\"labeling class_name\"+'%s/%s'%(dir,str(class_name)) + \"as \" '%s'%index)\n",
    "            #appending index to label_array \n",
    "            labelname = index\n",
    "            for pic_name in os.listdir(abs_path_class_name):\n",
    "                abs_path_pic_name = abs_path_class_name + \"/\" + pic_name\n",
    "                pic_array = read_image_as_array(abs_path_pic_name)\n",
    "                big_array.append(pic_array)\n",
    "                print(np.asarray(big_array).shape)\n",
    "                label_array.append(labelname)\n",
    "        else:\n",
    "            print(\"not folder, %s\" % class_name)\n",
    "    #turn python array to numpy\n",
    "    return (np.asarray(big_array), np.asarray(label_array))\n",
    "\n",
    "def read_image_as_array(dir):\n",
    "    \"\"\"\n",
    "    :param dir: directory of image name\n",
    "    :return array\n",
    "    \"\"\"\n",
    "\n",
    "    im = cv2.imread(dir)\n",
    "    #     if im.shape[0] != 56 and im.shape[1] != 56:\n",
    "    #         im = cv2.resize(im, (56, 56))\n",
    "    im = cv2.resize(im, (56, 56))\n",
    "    im = im.tolist()\n",
    "    return im\n",
    "\n",
    "def data_generate(cur_path):\n",
    "    \"\"\"\n",
    "    :param cur_path: input directory of train, validation data \n",
    "    :return (x_train, y_train), (x_test, y_test)\n",
    "    \"\"\"\n",
    "    (x_train, y_train) = make_label_for_each_directory(cur_path + \"/train\")\n",
    "    (x_test, y_test) = make_label_for_each_directory(cur_path + \"/validation\")\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "data_directory = \"data\"\n",
    "cur_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "(x_train, y_train), (x_test, y_test) = data_generate(cur_path + \"/\" + data_directory)\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "# x_train shape: (50000, 32, 32, 3)\n",
    "# y_train shape: (50000, 1)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Reference\n",
    "# http://stackoverflow.com/questions/29839350/numpy-append-vs-python-append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not folder, .DS_Store\n",
      "labeling class_name/kerasTest/data/train/0as 1\n",
      "labeling class_name/kerasTest/data/train/1as 2\n",
      "not folder, .DS_Store\n",
      "labeling class_name/kerasTest/data/validation/0as 1\n",
      "labeling class_name/kerasTest/data/validation/1as 2\n",
      "saving to csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-118352c30867>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving to csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x_train\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 297\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m   5490\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5491\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5492\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must pass 2-d input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "# import urllib.parse # for python 3\n",
    "from urlparse import urlparse # for python 2\n",
    "from itertools import count\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "data/\n",
    "    train/\n",
    "        0/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        1/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        0/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        1/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "\"\"\"\n",
    "\n",
    "def make_label_for_each_directory(dir):\n",
    "    \"\"\"\n",
    "    :param dir\n",
    "    :return (big_array, label_array): (nparray, label)\n",
    "    \"\"\"\n",
    "\n",
    "    big_array = []\n",
    "    label_array = []\n",
    "    for index, class_name in enumerate(os.listdir(dir), start=0):\n",
    "        abs_path_class_name = dir + \"/\" + class_name\n",
    "        if(os.path.isdir(abs_path_class_name)):\n",
    "            print(\"labeling class_name\"+'%s/%s'%(dir,str(class_name)) + \"as \" '%s'%index)\n",
    "            #appending index to label_array \n",
    "            labelname = index\n",
    "            for pic_name in os.listdir(abs_path_class_name):\n",
    "                abs_path_pic_name = abs_path_class_name + \"/\" + pic_name\n",
    "                if(verify(abs_path_pic_name)):\n",
    "                    pic_array = read_image_as_array(abs_path_pic_name)\n",
    "                    big_array.append(pic_array)\n",
    "#                     print(np.asarray(big_array).shape)\n",
    "                    label_array.append(labelname)\n",
    "        else:\n",
    "            print(\"not folder, %s\" % class_name)\n",
    "    #turn python array to numpy\n",
    "    return (np.asarray(big_array), np.asarray(label_array))\n",
    "\n",
    "def read_image_as_array(dir):\n",
    "    \"\"\"\n",
    "    :param dir: directory of image name\n",
    "    :return array\n",
    "    \"\"\"\n",
    "\n",
    "    im = cv2.imread(dir)\n",
    "    if im.shape[0] != 56 and im.shape[1] != 56:\n",
    "        im = cv2.resize(im, (56, 56))\n",
    "    im = im.tolist()\n",
    "    return im\n",
    "\n",
    "def data_generate(cur_path):\n",
    "    \"\"\"\n",
    "    :param cur_path: input directory of train, validation data \n",
    "    :return (x_train, y_train), (x_test, y_test)\n",
    "    \"\"\"\n",
    "    (x_train, y_train) = make_label_for_each_directory(cur_path + \"/train\")\n",
    "    (x_test, y_test) = make_label_for_each_directory(cur_path + \"/validation\")\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def verify(img_name):\n",
    "    \"\"\"\n",
    "    :param img_name: \n",
    "    :return Verify if img_name is img \n",
    "    \"\"\"\n",
    "    try:\n",
    "        im=Image.open(img_name)\n",
    "        return True\n",
    "    except IOError:\n",
    "        return False\n",
    "\n",
    "\n",
    "data_directory = \"data\"\n",
    "cur_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "(x_train, y_train), (x_test, y_test) = data_generate(cur_path + \"/\" + data_directory)\n",
    "\n",
    "\n",
    "print(\"saving to csv\")\n",
    "\n",
    "df = pd.DataFrame(x_train)\n",
    "df.to_csv(\"x_train\"+\".csv\")\n",
    "\n",
    "df = pd.DataFrame(y_train)\n",
    "df.to_csv(\"y_train\"+\".csv\")\n",
    "\n",
    "df = pd.DataFrame(x_test)\n",
    "df.to_csv(\"x_test\"+\".csv\")\n",
    "\n",
    "df = pd.DataFrame(y_test)\n",
    "df.to_csv(\"y_test\"+\".csv\")\n",
    "\n",
    "print(\"finished saving to csv\")\n",
    "\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "# x_train shape: (50000, 32, 32, 3)\n",
    "# y_train shape: (50000, 1)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "np.savetxt('test.csv', x_train, delimiter=',')\n",
    "ndarr2 = np.loadtxt('test.csv', delimiter=',')\n",
    "# Reference\n",
    "# http://stackoverflow.com/questions/29839350/numpy-append-vs-python-append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train\n",
      "y_train\n",
      "x_test\n",
      "y_test\n"
     ]
    }
   ],
   "source": [
    "for i in (\"x_train\", \"y_train\", \"x_test\", \"y_test\"):\n",
    "    print i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
