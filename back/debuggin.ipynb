{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named input_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-25d5142a32fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# input_data.pyはチュートリアル内にリンクがあるのでそこから取得する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/examples/tutorials/mnist/input_data.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named input_data"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# TensowFlowのインポート\n",
    "import tensorflow as tf\n",
    "# MNISTを読み込むためinput_data.pyを同じディレクトリに置きインポートする\n",
    "# input_data.pyはチュートリアル内にリンクがあるのでそこから取得する\n",
    "# https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "import input_data\n",
    "\n",
    "import time\n",
    "\n",
    "# 開始時刻\n",
    "start_time = time.time()\n",
    "print \"開始時刻: \" + str(start_time)\n",
    "\n",
    "# MNISTデータの読み込み\n",
    "# 60000点の訓練データ（mnist.train）と10000点のテストデータ（mnist.test）がある\n",
    "# 訓練データとテストデータにはそれぞれ0-9の画像とそれに対応するラベル（0-9）がある\n",
    "# 画像は28x28px(=784)のサイズ\n",
    "# mnist.train.imagesは[60000, 784]の配列であり、mnist.train.lablesは[60000, 10]の配列\n",
    "# lablesの配列は、対応するimagesの画像が3の数字であるならば、[0,0,0,1,0,0,0,0,0,0]となっている\n",
    "# mnist.test.imagesは[10000, 784]の配列であり、mnist.test.lablesは[10000, 10]の配列\n",
    "print \"--- MNISTデータの読み込み開始 ---\"\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print \"--- MNISTデータの読み込み完了 ---\"\n",
    "\n",
    "# 訓練画像を入れる変数\n",
    "# 訓練画像は28x28pxであり、これらを1行784列のベクトルに並び替え格納する\n",
    "# Noneとなっているのは訓練画像がいくつでも入れられるようにするため\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# 重み\n",
    "# 訓練画像のpx数の行、ラベル（0-9の数字の個数）数の列の行列\n",
    "# 初期値として0を入れておく\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "# バイアス\n",
    "# ラベル数の列の行列\n",
    "# 初期値として0を入れておく\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# ソフトマックス回帰を実行\n",
    "# yは入力x（画像）に対しそれがある数字である確率の分布\n",
    "# matmul関数で行列xとWの掛け算を行った後、bを加算する。\n",
    "# yは[1, 10]の行列\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 交差エントロピー\n",
    "# y_は正解データのラベル\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# 勾配硬化法を用い交差エントロピーが最小となるようyを最適化する\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# 用意した変数Veriableの初期化を実行する\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Sessionを開始する\n",
    "# runすることで初めて実行開始される（run(init)しないとinitが実行されない）\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 1000回の訓練（train_step）を実行する\n",
    "# next_batch(100)で100つのランダムな訓練セット（画像と対応するラベル）を選択する\n",
    "# 訓練データは60000点あるので全て使いたいところだが費用つまり時間がかかるのでランダムな100つを使う\n",
    "# 100つでも同じような結果を得ることができる\n",
    "# feed_dictでplaceholderに値を入力することができる\n",
    "print \"--- 訓練開始 ---\"\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_:batch_ys})\n",
    "print \"--- 訓練終了 ---\"\n",
    "\n",
    "# 正しいかの予測\n",
    "# 計算された画像がどの数字であるかの予測yと正解ラベルy_を比較する\n",
    "# 同じ値であればTrueが返される\n",
    "# argmaxは配列の中で一番値の大きい箇所のindexが返される\n",
    "# 一番値が大きいindexということは、それがその数字である確率が一番大きいということ\n",
    "# Trueが返ってくるということは訓練した結果と回答が同じということ\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "# 精度の計算\n",
    "# correct_predictionはbooleanなのでfloatにキャストし、平均値を計算する\n",
    "# Trueならば1、Falseならば0に変換される\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# 精度の実行と表示\n",
    "# テストデータの画像とラベルで精度を確認する\n",
    "# ソフトマックス回帰によってWとbの値が計算されているので、xを入力することでyが計算できる\n",
    "print \"精度\"\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "# 終了時刻\n",
    "end_time = time.time()\n",
    "print \"終了時刻: \" + str(end_time)\n",
    "print \"かかった時間: \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 images belonging to 2 classes.\n",
      "Found 41 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      " 728/1680 [============>.................] - ETA: 29s - loss: 0.1420 - acc: 0.7720\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os.path\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 56, 56\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "\n",
    "f_log = './log'\n",
    "#model saving location\n",
    "f_model = './model'\n",
    "model_filename = 'cnn_model.json'\n",
    "# weights_filename = 'cnn_model_weights.hdf5' \n",
    "weights_filename = 'weights.hdf5'\n",
    "\n",
    "train_sample_number = 56\n",
    "test_sample_number = 41\n",
    "\n",
    "nb_train_samples = train_sample_number*30\n",
    "nb_validation_samples = test_sample_number*20\n",
    "train_batch = train_sample_number\n",
    "test_batch = test_sample_number\n",
    "nb_epoch = 20\n",
    "\n",
    "\n",
    "#training model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3,border_mode='valid', input_shape=(img_width, img_height, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=5e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    zca_whitening=True,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=180, \n",
    "    width_shift_range=0.3,\n",
    "\theight_shift_range=0.3,\n",
    "\tfill_mode='nearest',\n",
    "\tvertical_flip=True,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(\n",
    "\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=180, \n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=train_batch,\n",
    "\n",
    "\t# save_to_dir=\"saved\",   \n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=test_batch,\n",
    "        class_mode='binary')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./saved/weights.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples,\n",
    "        callbacks=[checkpointer])\n",
    "\n",
    "\n",
    "print('save the architecture of a model')\n",
    "json_string = model.to_json()\n",
    "open(os.path.join(f_model,'cnn_model.json'), 'w').write(json_string)\n",
    "yaml_string = model.to_yaml()\n",
    "open(os.path.join(f_model,'cnn_model.yaml'), 'w').write(yaml_string)\n",
    "print('save weights')\n",
    "model.save_weights(os.path.join(f_model,'cnn_model_weights.hdf5'))\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
